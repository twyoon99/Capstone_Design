{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUlxf-b10Xur",
        "outputId": "f9a3b940-a11f-4742-e513-79118b2f59ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import models\n",
        "from keras.callbacks import EarlyStopping, CSVLogger, LearningRateScheduler, ModelCheckpoint\n",
        "from keras.layers import Dense, Flatten, Input, Dropout, BatchNormalization\n",
        "from keras.models import load_model, Model\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "3RYZ6Gt77Tud"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/MyDrive/캡스톤디자인I/EEG_DATA_Shuffle_500.csv'\n",
        "dataDF = pd.read_csv(path)\n",
        "dataDF = dataDF[dataDF.iloc[:, -1] != 2]\n",
        "dataDF = dataDF.drop('Unnamed: 0', axis=1)\n",
        "dataDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "ldRUF4Bw0iOX",
        "outputId": "422778ea-aa22-4310-b7c4-188d8629654a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0        1        2        3        4        5        6        7  \\\n",
              "0      7.7005  16.5688  14.6191  -2.7372   9.7376   9.6343  12.7423  15.5180   \n",
              "3    -76.7901  10.4661  60.9218  39.1930  33.3773  65.0877  49.0782   5.1342   \n",
              "4      9.5458   8.5632   2.2690  -7.9035   7.0756   7.6126   9.5961   8.0069   \n",
              "5     54.1066 -29.2396 -52.1869 -46.1536 -39.8975 -55.1390 -20.3230  20.2025   \n",
              "6    -21.9437 -34.9235 -27.5263 -20.9710  -0.9563   3.7334  -7.1844 -14.7087   \n",
              "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
              "3370  31.8298  28.2079  16.0677  16.1556  17.7885  10.1546   5.3282   8.8298   \n",
              "3371  -7.0424 -19.2560   1.3570  -6.2609 -12.3790 -10.1178 -10.6833   2.2586   \n",
              "3372 -34.5889 -16.0601   3.5147  56.5993 -13.7713  -1.4305  18.7316  -2.2079   \n",
              "3373  -7.7020 -32.1352 -19.0261  18.9835 -12.2132 -22.5264  -5.8123  -6.4590   \n",
              "3374  -4.6335  -5.4430  -7.4076  -8.2689  -6.3562  -7.0784  -4.4641  -4.4485   \n",
              "\n",
              "             8        9  ...     9990     9991     9992     9993     9994  \\\n",
              "0      23.1480   3.2137  ...  20.2932  19.9336  26.9401   0.4877  15.9825   \n",
              "3      43.5867   0.9796  ... -38.8314 -50.8898 -42.2924 -22.9204 -48.7766   \n",
              "4      60.6086  -3.4619  ... -17.7538  -3.9169 -11.5917  -7.9538 -12.3448   \n",
              "5     -16.8425  16.3951  ... -58.7427 -43.0384 -24.9481 -13.5026 -74.4552   \n",
              "6       3.9823 -16.8421  ...   0.6112  -1.8943  -4.1084   6.3730  -5.9389   \n",
              "...        ...      ...  ...      ...      ...      ...      ...      ...   \n",
              "3370   -6.3858 -37.7427  ... -13.2528 -33.0454 -21.1263  -9.9571 -86.3534   \n",
              "3371    3.8605 -12.5384  ...   1.5050 -16.4925 -26.9612 -13.1342 -21.2572   \n",
              "3372    3.7468   0.6465  ...   2.6975  20.9829  10.2222   4.9892  -9.2410   \n",
              "3373 -868.1081   0.2058  ...  -1.5312 -23.7726 -16.1596   3.1612   0.3230   \n",
              "3374    4.3011  -7.1006  ... -16.9479 -23.1283 -29.2086 -20.7122 -17.9857   \n",
              "\n",
              "         9995     9996     9997     9998  9999  \n",
              "0     15.2125  28.8786  13.1696  29.3290   1.0  \n",
              "3    -44.7011 -38.4557 -40.1042 -36.8672   1.0  \n",
              "4    -18.7311  -4.8867 -10.0855   5.3741   1.0  \n",
              "5    -17.3629 -48.7126 -28.2805 -16.2469   0.0  \n",
              "6      8.8530  -0.3462 -14.9738 -15.7967   0.0  \n",
              "...       ...      ...      ...      ...   ...  \n",
              "3370 -24.2241 -32.5561  -5.7018   3.6393   1.0  \n",
              "3371  -1.3507 -27.7396  -6.2248  -9.7443   1.0  \n",
              "3372   3.5549  13.9819  -3.4836  -1.3360   0.0  \n",
              "3373   6.0892 -14.2523   3.1717 -37.0443   4.0  \n",
              "3374  -9.2372 -14.3896 -35.0393 -41.9177   1.0  \n",
              "\n",
              "[2700 rows x 10000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e00cd9e1-b7cb-4ae6-aaf3-16f46c4c1493\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>9990</th>\n",
              "      <th>9991</th>\n",
              "      <th>9992</th>\n",
              "      <th>9993</th>\n",
              "      <th>9994</th>\n",
              "      <th>9995</th>\n",
              "      <th>9996</th>\n",
              "      <th>9997</th>\n",
              "      <th>9998</th>\n",
              "      <th>9999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.7005</td>\n",
              "      <td>16.5688</td>\n",
              "      <td>14.6191</td>\n",
              "      <td>-2.7372</td>\n",
              "      <td>9.7376</td>\n",
              "      <td>9.6343</td>\n",
              "      <td>12.7423</td>\n",
              "      <td>15.5180</td>\n",
              "      <td>23.1480</td>\n",
              "      <td>3.2137</td>\n",
              "      <td>...</td>\n",
              "      <td>20.2932</td>\n",
              "      <td>19.9336</td>\n",
              "      <td>26.9401</td>\n",
              "      <td>0.4877</td>\n",
              "      <td>15.9825</td>\n",
              "      <td>15.2125</td>\n",
              "      <td>28.8786</td>\n",
              "      <td>13.1696</td>\n",
              "      <td>29.3290</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-76.7901</td>\n",
              "      <td>10.4661</td>\n",
              "      <td>60.9218</td>\n",
              "      <td>39.1930</td>\n",
              "      <td>33.3773</td>\n",
              "      <td>65.0877</td>\n",
              "      <td>49.0782</td>\n",
              "      <td>5.1342</td>\n",
              "      <td>43.5867</td>\n",
              "      <td>0.9796</td>\n",
              "      <td>...</td>\n",
              "      <td>-38.8314</td>\n",
              "      <td>-50.8898</td>\n",
              "      <td>-42.2924</td>\n",
              "      <td>-22.9204</td>\n",
              "      <td>-48.7766</td>\n",
              "      <td>-44.7011</td>\n",
              "      <td>-38.4557</td>\n",
              "      <td>-40.1042</td>\n",
              "      <td>-36.8672</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9.5458</td>\n",
              "      <td>8.5632</td>\n",
              "      <td>2.2690</td>\n",
              "      <td>-7.9035</td>\n",
              "      <td>7.0756</td>\n",
              "      <td>7.6126</td>\n",
              "      <td>9.5961</td>\n",
              "      <td>8.0069</td>\n",
              "      <td>60.6086</td>\n",
              "      <td>-3.4619</td>\n",
              "      <td>...</td>\n",
              "      <td>-17.7538</td>\n",
              "      <td>-3.9169</td>\n",
              "      <td>-11.5917</td>\n",
              "      <td>-7.9538</td>\n",
              "      <td>-12.3448</td>\n",
              "      <td>-18.7311</td>\n",
              "      <td>-4.8867</td>\n",
              "      <td>-10.0855</td>\n",
              "      <td>5.3741</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>54.1066</td>\n",
              "      <td>-29.2396</td>\n",
              "      <td>-52.1869</td>\n",
              "      <td>-46.1536</td>\n",
              "      <td>-39.8975</td>\n",
              "      <td>-55.1390</td>\n",
              "      <td>-20.3230</td>\n",
              "      <td>20.2025</td>\n",
              "      <td>-16.8425</td>\n",
              "      <td>16.3951</td>\n",
              "      <td>...</td>\n",
              "      <td>-58.7427</td>\n",
              "      <td>-43.0384</td>\n",
              "      <td>-24.9481</td>\n",
              "      <td>-13.5026</td>\n",
              "      <td>-74.4552</td>\n",
              "      <td>-17.3629</td>\n",
              "      <td>-48.7126</td>\n",
              "      <td>-28.2805</td>\n",
              "      <td>-16.2469</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-21.9437</td>\n",
              "      <td>-34.9235</td>\n",
              "      <td>-27.5263</td>\n",
              "      <td>-20.9710</td>\n",
              "      <td>-0.9563</td>\n",
              "      <td>3.7334</td>\n",
              "      <td>-7.1844</td>\n",
              "      <td>-14.7087</td>\n",
              "      <td>3.9823</td>\n",
              "      <td>-16.8421</td>\n",
              "      <td>...</td>\n",
              "      <td>0.6112</td>\n",
              "      <td>-1.8943</td>\n",
              "      <td>-4.1084</td>\n",
              "      <td>6.3730</td>\n",
              "      <td>-5.9389</td>\n",
              "      <td>8.8530</td>\n",
              "      <td>-0.3462</td>\n",
              "      <td>-14.9738</td>\n",
              "      <td>-15.7967</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3370</th>\n",
              "      <td>31.8298</td>\n",
              "      <td>28.2079</td>\n",
              "      <td>16.0677</td>\n",
              "      <td>16.1556</td>\n",
              "      <td>17.7885</td>\n",
              "      <td>10.1546</td>\n",
              "      <td>5.3282</td>\n",
              "      <td>8.8298</td>\n",
              "      <td>-6.3858</td>\n",
              "      <td>-37.7427</td>\n",
              "      <td>...</td>\n",
              "      <td>-13.2528</td>\n",
              "      <td>-33.0454</td>\n",
              "      <td>-21.1263</td>\n",
              "      <td>-9.9571</td>\n",
              "      <td>-86.3534</td>\n",
              "      <td>-24.2241</td>\n",
              "      <td>-32.5561</td>\n",
              "      <td>-5.7018</td>\n",
              "      <td>3.6393</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3371</th>\n",
              "      <td>-7.0424</td>\n",
              "      <td>-19.2560</td>\n",
              "      <td>1.3570</td>\n",
              "      <td>-6.2609</td>\n",
              "      <td>-12.3790</td>\n",
              "      <td>-10.1178</td>\n",
              "      <td>-10.6833</td>\n",
              "      <td>2.2586</td>\n",
              "      <td>3.8605</td>\n",
              "      <td>-12.5384</td>\n",
              "      <td>...</td>\n",
              "      <td>1.5050</td>\n",
              "      <td>-16.4925</td>\n",
              "      <td>-26.9612</td>\n",
              "      <td>-13.1342</td>\n",
              "      <td>-21.2572</td>\n",
              "      <td>-1.3507</td>\n",
              "      <td>-27.7396</td>\n",
              "      <td>-6.2248</td>\n",
              "      <td>-9.7443</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3372</th>\n",
              "      <td>-34.5889</td>\n",
              "      <td>-16.0601</td>\n",
              "      <td>3.5147</td>\n",
              "      <td>56.5993</td>\n",
              "      <td>-13.7713</td>\n",
              "      <td>-1.4305</td>\n",
              "      <td>18.7316</td>\n",
              "      <td>-2.2079</td>\n",
              "      <td>3.7468</td>\n",
              "      <td>0.6465</td>\n",
              "      <td>...</td>\n",
              "      <td>2.6975</td>\n",
              "      <td>20.9829</td>\n",
              "      <td>10.2222</td>\n",
              "      <td>4.9892</td>\n",
              "      <td>-9.2410</td>\n",
              "      <td>3.5549</td>\n",
              "      <td>13.9819</td>\n",
              "      <td>-3.4836</td>\n",
              "      <td>-1.3360</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3373</th>\n",
              "      <td>-7.7020</td>\n",
              "      <td>-32.1352</td>\n",
              "      <td>-19.0261</td>\n",
              "      <td>18.9835</td>\n",
              "      <td>-12.2132</td>\n",
              "      <td>-22.5264</td>\n",
              "      <td>-5.8123</td>\n",
              "      <td>-6.4590</td>\n",
              "      <td>-868.1081</td>\n",
              "      <td>0.2058</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.5312</td>\n",
              "      <td>-23.7726</td>\n",
              "      <td>-16.1596</td>\n",
              "      <td>3.1612</td>\n",
              "      <td>0.3230</td>\n",
              "      <td>6.0892</td>\n",
              "      <td>-14.2523</td>\n",
              "      <td>3.1717</td>\n",
              "      <td>-37.0443</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3374</th>\n",
              "      <td>-4.6335</td>\n",
              "      <td>-5.4430</td>\n",
              "      <td>-7.4076</td>\n",
              "      <td>-8.2689</td>\n",
              "      <td>-6.3562</td>\n",
              "      <td>-7.0784</td>\n",
              "      <td>-4.4641</td>\n",
              "      <td>-4.4485</td>\n",
              "      <td>4.3011</td>\n",
              "      <td>-7.1006</td>\n",
              "      <td>...</td>\n",
              "      <td>-16.9479</td>\n",
              "      <td>-23.1283</td>\n",
              "      <td>-29.2086</td>\n",
              "      <td>-20.7122</td>\n",
              "      <td>-17.9857</td>\n",
              "      <td>-9.2372</td>\n",
              "      <td>-14.3896</td>\n",
              "      <td>-35.0393</td>\n",
              "      <td>-41.9177</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2700 rows × 10000 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e00cd9e1-b7cb-4ae6-aaf3-16f46c4c1493')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e00cd9e1-b7cb-4ae6-aaf3-16f46c4c1493 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e00cd9e1-b7cb-4ae6-aaf3-16f46c4c1493');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c143a5b9-e5dd-46cc-9662-c8838796d446\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c143a5b9-e5dd-46cc-9662-c8838796d446')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c143a5b9-e5dd-46cc-9662-c8838796d446 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_48cdeb7a-226f-4110-a4e7-0cc3b6fe3d8b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataDF')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_48cdeb7a-226f-4110-a4e7-0cc3b6fe3d8b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dataDF');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataDF"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ct = 500\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "print(len(dataDF))\n",
        "\n",
        "for i in range(len(dataDF)):\n",
        "    # for i in range(100):\n",
        "    print(i)\n",
        "    x_data.append([])\n",
        "    x = 0\n",
        "    y = 20\n",
        "    one_row_data = dataDF.iloc[i]\n",
        "    y_data.append(one_row_data[y - 1])\n",
        "    for j in range(ct):\n",
        "        x_data[i].append([one_row_data[x:y - 1]])\n",
        "        x += 20\n",
        "        y += 20\n",
        "\n",
        "x_data = np.array(x_data).reshape(len(x_data), ct, 19)\n",
        "y_data = np.array(pd.get_dummies(y_data))\n",
        "\n",
        "x_data_train, x_data_test, y_data_train, y_data_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2024)\n",
        "\n",
        "x_data_train, x_data_val, y_data_train, y_data_val = train_test_split(x_data_train, y_data_train, test_size=0.25, random_state=2024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86m4dWceDH9r",
        "outputId": "e1210600-cb60-4159-e8ca-dc7ce1b045e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2700\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n",
            "1435\n",
            "1436\n",
            "1437\n",
            "1438\n",
            "1439\n",
            "1440\n",
            "1441\n",
            "1442\n",
            "1443\n",
            "1444\n",
            "1445\n",
            "1446\n",
            "1447\n",
            "1448\n",
            "1449\n",
            "1450\n",
            "1451\n",
            "1452\n",
            "1453\n",
            "1454\n",
            "1455\n",
            "1456\n",
            "1457\n",
            "1458\n",
            "1459\n",
            "1460\n",
            "1461\n",
            "1462\n",
            "1463\n",
            "1464\n",
            "1465\n",
            "1466\n",
            "1467\n",
            "1468\n",
            "1469\n",
            "1470\n",
            "1471\n",
            "1472\n",
            "1473\n",
            "1474\n",
            "1475\n",
            "1476\n",
            "1477\n",
            "1478\n",
            "1479\n",
            "1480\n",
            "1481\n",
            "1482\n",
            "1483\n",
            "1484\n",
            "1485\n",
            "1486\n",
            "1487\n",
            "1488\n",
            "1489\n",
            "1490\n",
            "1491\n",
            "1492\n",
            "1493\n",
            "1494\n",
            "1495\n",
            "1496\n",
            "1497\n",
            "1498\n",
            "1499\n",
            "1500\n",
            "1501\n",
            "1502\n",
            "1503\n",
            "1504\n",
            "1505\n",
            "1506\n",
            "1507\n",
            "1508\n",
            "1509\n",
            "1510\n",
            "1511\n",
            "1512\n",
            "1513\n",
            "1514\n",
            "1515\n",
            "1516\n",
            "1517\n",
            "1518\n",
            "1519\n",
            "1520\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1526\n",
            "1527\n",
            "1528\n",
            "1529\n",
            "1530\n",
            "1531\n",
            "1532\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1536\n",
            "1537\n",
            "1538\n",
            "1539\n",
            "1540\n",
            "1541\n",
            "1542\n",
            "1543\n",
            "1544\n",
            "1545\n",
            "1546\n",
            "1547\n",
            "1548\n",
            "1549\n",
            "1550\n",
            "1551\n",
            "1552\n",
            "1553\n",
            "1554\n",
            "1555\n",
            "1556\n",
            "1557\n",
            "1558\n",
            "1559\n",
            "1560\n",
            "1561\n",
            "1562\n",
            "1563\n",
            "1564\n",
            "1565\n",
            "1566\n",
            "1567\n",
            "1568\n",
            "1569\n",
            "1570\n",
            "1571\n",
            "1572\n",
            "1573\n",
            "1574\n",
            "1575\n",
            "1576\n",
            "1577\n",
            "1578\n",
            "1579\n",
            "1580\n",
            "1581\n",
            "1582\n",
            "1583\n",
            "1584\n",
            "1585\n",
            "1586\n",
            "1587\n",
            "1588\n",
            "1589\n",
            "1590\n",
            "1591\n",
            "1592\n",
            "1593\n",
            "1594\n",
            "1595\n",
            "1596\n",
            "1597\n",
            "1598\n",
            "1599\n",
            "1600\n",
            "1601\n",
            "1602\n",
            "1603\n",
            "1604\n",
            "1605\n",
            "1606\n",
            "1607\n",
            "1608\n",
            "1609\n",
            "1610\n",
            "1611\n",
            "1612\n",
            "1613\n",
            "1614\n",
            "1615\n",
            "1616\n",
            "1617\n",
            "1618\n",
            "1619\n",
            "1620\n",
            "1621\n",
            "1622\n",
            "1623\n",
            "1624\n",
            "1625\n",
            "1626\n",
            "1627\n",
            "1628\n",
            "1629\n",
            "1630\n",
            "1631\n",
            "1632\n",
            "1633\n",
            "1634\n",
            "1635\n",
            "1636\n",
            "1637\n",
            "1638\n",
            "1639\n",
            "1640\n",
            "1641\n",
            "1642\n",
            "1643\n",
            "1644\n",
            "1645\n",
            "1646\n",
            "1647\n",
            "1648\n",
            "1649\n",
            "1650\n",
            "1651\n",
            "1652\n",
            "1653\n",
            "1654\n",
            "1655\n",
            "1656\n",
            "1657\n",
            "1658\n",
            "1659\n",
            "1660\n",
            "1661\n",
            "1662\n",
            "1663\n",
            "1664\n",
            "1665\n",
            "1666\n",
            "1667\n",
            "1668\n",
            "1669\n",
            "1670\n",
            "1671\n",
            "1672\n",
            "1673\n",
            "1674\n",
            "1675\n",
            "1676\n",
            "1677\n",
            "1678\n",
            "1679\n",
            "1680\n",
            "1681\n",
            "1682\n",
            "1683\n",
            "1684\n",
            "1685\n",
            "1686\n",
            "1687\n",
            "1688\n",
            "1689\n",
            "1690\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1695\n",
            "1696\n",
            "1697\n",
            "1698\n",
            "1699\n",
            "1700\n",
            "1701\n",
            "1702\n",
            "1703\n",
            "1704\n",
            "1705\n",
            "1706\n",
            "1707\n",
            "1708\n",
            "1709\n",
            "1710\n",
            "1711\n",
            "1712\n",
            "1713\n",
            "1714\n",
            "1715\n",
            "1716\n",
            "1717\n",
            "1718\n",
            "1719\n",
            "1720\n",
            "1721\n",
            "1722\n",
            "1723\n",
            "1724\n",
            "1725\n",
            "1726\n",
            "1727\n",
            "1728\n",
            "1729\n",
            "1730\n",
            "1731\n",
            "1732\n",
            "1733\n",
            "1734\n",
            "1735\n",
            "1736\n",
            "1737\n",
            "1738\n",
            "1739\n",
            "1740\n",
            "1741\n",
            "1742\n",
            "1743\n",
            "1744\n",
            "1745\n",
            "1746\n",
            "1747\n",
            "1748\n",
            "1749\n",
            "1750\n",
            "1751\n",
            "1752\n",
            "1753\n",
            "1754\n",
            "1755\n",
            "1756\n",
            "1757\n",
            "1758\n",
            "1759\n",
            "1760\n",
            "1761\n",
            "1762\n",
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "1800\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "1900\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "2000\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "2100\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "2200\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "2300\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n",
            "2325\n",
            "2326\n",
            "2327\n",
            "2328\n",
            "2329\n",
            "2330\n",
            "2331\n",
            "2332\n",
            "2333\n",
            "2334\n",
            "2335\n",
            "2336\n",
            "2337\n",
            "2338\n",
            "2339\n",
            "2340\n",
            "2341\n",
            "2342\n",
            "2343\n",
            "2344\n",
            "2345\n",
            "2346\n",
            "2347\n",
            "2348\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2352\n",
            "2353\n",
            "2354\n",
            "2355\n",
            "2356\n",
            "2357\n",
            "2358\n",
            "2359\n",
            "2360\n",
            "2361\n",
            "2362\n",
            "2363\n",
            "2364\n",
            "2365\n",
            "2366\n",
            "2367\n",
            "2368\n",
            "2369\n",
            "2370\n",
            "2371\n",
            "2372\n",
            "2373\n",
            "2374\n",
            "2375\n",
            "2376\n",
            "2377\n",
            "2378\n",
            "2379\n",
            "2380\n",
            "2381\n",
            "2382\n",
            "2383\n",
            "2384\n",
            "2385\n",
            "2386\n",
            "2387\n",
            "2388\n",
            "2389\n",
            "2390\n",
            "2391\n",
            "2392\n",
            "2393\n",
            "2394\n",
            "2395\n",
            "2396\n",
            "2397\n",
            "2398\n",
            "2399\n",
            "2400\n",
            "2401\n",
            "2402\n",
            "2403\n",
            "2404\n",
            "2405\n",
            "2406\n",
            "2407\n",
            "2408\n",
            "2409\n",
            "2410\n",
            "2411\n",
            "2412\n",
            "2413\n",
            "2414\n",
            "2415\n",
            "2416\n",
            "2417\n",
            "2418\n",
            "2419\n",
            "2420\n",
            "2421\n",
            "2422\n",
            "2423\n",
            "2424\n",
            "2425\n",
            "2426\n",
            "2427\n",
            "2428\n",
            "2429\n",
            "2430\n",
            "2431\n",
            "2432\n",
            "2433\n",
            "2434\n",
            "2435\n",
            "2436\n",
            "2437\n",
            "2438\n",
            "2439\n",
            "2440\n",
            "2441\n",
            "2442\n",
            "2443\n",
            "2444\n",
            "2445\n",
            "2446\n",
            "2447\n",
            "2448\n",
            "2449\n",
            "2450\n",
            "2451\n",
            "2452\n",
            "2453\n",
            "2454\n",
            "2455\n",
            "2456\n",
            "2457\n",
            "2458\n",
            "2459\n",
            "2460\n",
            "2461\n",
            "2462\n",
            "2463\n",
            "2464\n",
            "2465\n",
            "2466\n",
            "2467\n",
            "2468\n",
            "2469\n",
            "2470\n",
            "2471\n",
            "2472\n",
            "2473\n",
            "2474\n",
            "2475\n",
            "2476\n",
            "2477\n",
            "2478\n",
            "2479\n",
            "2480\n",
            "2481\n",
            "2482\n",
            "2483\n",
            "2484\n",
            "2485\n",
            "2486\n",
            "2487\n",
            "2488\n",
            "2489\n",
            "2490\n",
            "2491\n",
            "2492\n",
            "2493\n",
            "2494\n",
            "2495\n",
            "2496\n",
            "2497\n",
            "2498\n",
            "2499\n",
            "2500\n",
            "2501\n",
            "2502\n",
            "2503\n",
            "2504\n",
            "2505\n",
            "2506\n",
            "2507\n",
            "2508\n",
            "2509\n",
            "2510\n",
            "2511\n",
            "2512\n",
            "2513\n",
            "2514\n",
            "2515\n",
            "2516\n",
            "2517\n",
            "2518\n",
            "2519\n",
            "2520\n",
            "2521\n",
            "2522\n",
            "2523\n",
            "2524\n",
            "2525\n",
            "2526\n",
            "2527\n",
            "2528\n",
            "2529\n",
            "2530\n",
            "2531\n",
            "2532\n",
            "2533\n",
            "2534\n",
            "2535\n",
            "2536\n",
            "2537\n",
            "2538\n",
            "2539\n",
            "2540\n",
            "2541\n",
            "2542\n",
            "2543\n",
            "2544\n",
            "2545\n",
            "2546\n",
            "2547\n",
            "2548\n",
            "2549\n",
            "2550\n",
            "2551\n",
            "2552\n",
            "2553\n",
            "2554\n",
            "2555\n",
            "2556\n",
            "2557\n",
            "2558\n",
            "2559\n",
            "2560\n",
            "2561\n",
            "2562\n",
            "2563\n",
            "2564\n",
            "2565\n",
            "2566\n",
            "2567\n",
            "2568\n",
            "2569\n",
            "2570\n",
            "2571\n",
            "2572\n",
            "2573\n",
            "2574\n",
            "2575\n",
            "2576\n",
            "2577\n",
            "2578\n",
            "2579\n",
            "2580\n",
            "2581\n",
            "2582\n",
            "2583\n",
            "2584\n",
            "2585\n",
            "2586\n",
            "2587\n",
            "2588\n",
            "2589\n",
            "2590\n",
            "2591\n",
            "2592\n",
            "2593\n",
            "2594\n",
            "2595\n",
            "2596\n",
            "2597\n",
            "2598\n",
            "2599\n",
            "2600\n",
            "2601\n",
            "2602\n",
            "2603\n",
            "2604\n",
            "2605\n",
            "2606\n",
            "2607\n",
            "2608\n",
            "2609\n",
            "2610\n",
            "2611\n",
            "2612\n",
            "2613\n",
            "2614\n",
            "2615\n",
            "2616\n",
            "2617\n",
            "2618\n",
            "2619\n",
            "2620\n",
            "2621\n",
            "2622\n",
            "2623\n",
            "2624\n",
            "2625\n",
            "2626\n",
            "2627\n",
            "2628\n",
            "2629\n",
            "2630\n",
            "2631\n",
            "2632\n",
            "2633\n",
            "2634\n",
            "2635\n",
            "2636\n",
            "2637\n",
            "2638\n",
            "2639\n",
            "2640\n",
            "2641\n",
            "2642\n",
            "2643\n",
            "2644\n",
            "2645\n",
            "2646\n",
            "2647\n",
            "2648\n",
            "2649\n",
            "2650\n",
            "2651\n",
            "2652\n",
            "2653\n",
            "2654\n",
            "2655\n",
            "2656\n",
            "2657\n",
            "2658\n",
            "2659\n",
            "2660\n",
            "2661\n",
            "2662\n",
            "2663\n",
            "2664\n",
            "2665\n",
            "2666\n",
            "2667\n",
            "2668\n",
            "2669\n",
            "2670\n",
            "2671\n",
            "2672\n",
            "2673\n",
            "2674\n",
            "2675\n",
            "2676\n",
            "2677\n",
            "2678\n",
            "2679\n",
            "2680\n",
            "2681\n",
            "2682\n",
            "2683\n",
            "2684\n",
            "2685\n",
            "2686\n",
            "2687\n",
            "2688\n",
            "2689\n",
            "2690\n",
            "2691\n",
            "2692\n",
            "2693\n",
            "2694\n",
            "2695\n",
            "2696\n",
            "2697\n",
            "2698\n",
            "2699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training set shapes:\")\n",
        "print(x_data_train.shape)\n",
        "print(y_data_train.shape)\n",
        "print(\"-------------------\")\n",
        "print(\"Validation set shapes:\")\n",
        "print(x_data_val.shape)\n",
        "print(y_data_val.shape)\n",
        "print(\"-------------------\")\n",
        "print(\"Test set shapes:\")\n",
        "print(x_data_test.shape)\n",
        "print(y_data_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzWuYe6mDYnY",
        "outputId": "e35632b9-f327-4a35-fc0f-8edd066cc508"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shapes:\n",
            "(1620, 500, 19)\n",
            "(1620, 4)\n",
            "-------------------\n",
            "Validation set shapes:\n",
            "(540, 500, 19)\n",
            "(540, 4)\n",
            "-------------------\n",
            "Test set shapes:\n",
            "(540, 500, 19)\n",
            "(540, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ct=500\n",
        "\n",
        "# x_data = []\n",
        "# y_data = []\n",
        "\n",
        "# print(len(dataDF))\n",
        "\n",
        "# for i in range(len(dataDF)):\n",
        "# # for i in range(100):\n",
        "#     print(i)\n",
        "#     x_data.append([])\n",
        "#     x = 0\n",
        "#     y = 20\n",
        "#     one_row_data = dataDF.iloc[i]\n",
        "#     y_data.append(one_row_data[y-1])\n",
        "#     for j in range(ct):\n",
        "#         x_data[i].append([one_row_data[x:y-1]])\n",
        "#         x += 20\n",
        "#         y += 20\n",
        "\n",
        "# x_data = np.array(x_data).reshape(len(x_data), ct, 19)\n",
        "# y_data = np.array(pd.get_dummies(y_data))\n",
        "\n",
        "# x_data, x_test_data, y_data, y_test_data = train_test_split(x_data, y_data, test_size=0.2, random_state=2024)\n",
        "\n",
        "# print('x_data :', x_data.shape)\n",
        "# print('y_data :', y_data.shape)\n",
        "# print('x_test_data :', x_test_data.shape)\n",
        "# print('y_test_data :', y_test_data.shape)"
      ],
      "metadata": {
        "id": "xKTfl2-G8P_Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 트랜스포머 Block 1개\n",
        "def transformer_block(input):\n",
        "  x = keras.layers.MultiHeadAttention(num_heads=16, key_dim=1, dropout=0.1)(input, input, return_attention_scores=False)\n",
        "  x = x + input\n",
        "  x = keras.layers.LayerNormalization()(x)\n",
        "  x2 = keras.layers.Dense(19,activation='relu')(x)\n",
        "  x2 = x + x2\n",
        "  x3 = keras.layers.LayerNormalization()(x2)\n",
        "  return x3"
      ],
      "metadata": {
        "id": "SvN6nGIJ8T4z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 transformer_block()을 사용해서 모델을 만드는 함수\n",
        "def build_model(num_block):\n",
        "  inputs = keras.Input((ct,19))\n",
        "  x = inputs\n",
        "  for _ in range(num_block):\n",
        "    x = transformer_block(x)\n",
        "  x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "  output = keras.layers.Dense(4,activation='softmax')(x)\n",
        "  return keras.Model(inputs,output)"
      ],
      "metadata": {
        "id": "e2DoDDYg8eTx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_block = 5\n",
        "myMdl = build_model(num_block)\n",
        "myMdl.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0-l-CbS8gqB",
        "outputId": "919566f7-3df9-4cec-c8b6-d66a73cf7ada"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 500, 19)]            0         []                            \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 500, 19)              1283      ['input_1[0][0]',             \n",
            " iHeadAttention)                                                     'input_1[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 500, 19)              0         ['multi_head_attention[0][0]',\n",
            " Lambda)                                                             'input_1[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 500, 19)              38        ['tf.__operators__.add[0][0]']\n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 500, 19)              380       ['layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TF  (None, 500, 19)              0         ['layer_normalization[0][0]', \n",
            " OpLambda)                                                           'dense[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 500, 19)              38        ['tf.__operators__.add_1[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 500, 19)              1283      ['layer_normalization_1[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TF  (None, 500, 19)              0         ['multi_head_attention_1[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 500, 19)              38        ['tf.__operators__.add_2[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 500, 19)              380       ['layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TF  (None, 500, 19)              0         ['layer_normalization_2[0][0]'\n",
            " OpLambda)                                                          , 'dense_1[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 500, 19)              38        ['tf.__operators__.add_3[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 500, 19)              1283      ['layer_normalization_3[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TF  (None, 500, 19)              0         ['multi_head_attention_2[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 500, 19)              38        ['tf.__operators__.add_4[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 500, 19)              380       ['layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TF  (None, 500, 19)              0         ['layer_normalization_4[0][0]'\n",
            " OpLambda)                                                          , 'dense_2[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 500, 19)              38        ['tf.__operators__.add_5[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 500, 19)              1283      ['layer_normalization_5[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TF  (None, 500, 19)              0         ['multi_head_attention_3[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 500, 19)              38        ['tf.__operators__.add_6[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 500, 19)              380       ['layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TF  (None, 500, 19)              0         ['layer_normalization_6[0][0]'\n",
            " OpLambda)                                                          , 'dense_3[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 500, 19)              38        ['tf.__operators__.add_7[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (Mu  (None, 500, 19)              1283      ['layer_normalization_7[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_7[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TF  (None, 500, 19)              0         ['multi_head_attention_4[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 500, 19)              38        ['tf.__operators__.add_8[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 500, 19)              380       ['layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TF  (None, 500, 19)              0         ['layer_normalization_8[0][0]'\n",
            " OpLambda)                                                          , 'dense_4[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 500, 19)              38        ['tf.__operators__.add_9[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d (  (None, 19)                   0         ['layer_normalization_9[0][0]'\n",
            " GlobalAveragePooling1D)                                            ]                             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 4)                    80        ['global_average_pooling1d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8775 (34.28 KB)\n",
            "Trainable params: 8775 (34.28 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #에포크마다 모델을 저장하고 loss를 기록한 csv 저장을 위한 코드\n",
        "# root = '/content/drive/MyDrive/캡스톤디자인I/Save_Model/'\n",
        "# folder_name = '20240411_1modelsave'\n",
        "# os.mkdir(root+folder_name)\n",
        "# csv_logger = CSVLogger(root+folder_name+'/training.csv', separator=\",\", append=True)\n",
        "# mcp_save_best = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     root+folder_name+'/{epoch:02d}-{val_loss:.5f}_best', save_best_only=True,\n",
        "#     monitor='val_loss', verbose=1, mode='min')\n",
        "# mcp_save = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     root+folder_name+'/{epoch:02d}-{val_loss:.5f}', save_best_only=False,\n",
        "#     monitor='val_loss', verbose=1, mode='min')"
      ],
      "metadata": {
        "id": "ClrJzF1n0prv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early_stop = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=5, restore_best_weights=True)\n",
        "# myMdl.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=['categorical_crossentropy'], metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Znq-9koW_r38"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #훈련 코드\n",
        "# trans_h = myMdl.fit(x_data, y_data, batch_size=64, epochs=500, validation_split=0.2)"
      ],
      "metadata": {
        "id": "SCP1UHP58nde"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = Adam(learning_rate=0.001)\n",
        "#옵티마이저를 Adam 알고리즘을 사용합니다. 학습률을 0.001로 설정(학습률은 가중치를 얼마나 업데이트할지 이를 조정하여 모델의 성능을 조정)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
        "mc_loss = ModelCheckpoint('/content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_loss_v4.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "#훈련중 손실값을 모니터링하여 조기 종료하는 콜백 함수\n",
        "mc = ModelCheckpoint('/content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_acc_v4.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "#모델의 검증 정확도를 확인하여 가장 좋은 시점의 모델을 저장함.\n",
        "lr_schedule = LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
        "#학습률을 조정하는 콜백 함수\n",
        "myMdl.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
        "#모델의 옵티마이저를 설정, 손실함수, 정확도를 출력\n",
        "trans_h = myMdl.fit(x_data_train, y_data_train,\n",
        "                   batch_size=64,\n",
        "                   validation_data=(x_data_val, y_data_val),\n",
        "                   epochs=200,\n",
        "                   callbacks=[es, mc, mc_loss, lr_schedule])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WpnDnmbG27o",
        "outputId": "1d83fb7e-604d-4eb7-948a-692aa7bd9a79"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9747\n",
            "Epoch 1: val_accuracy improved from -inf to 0.94259, saving model to /content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_acc_v4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.16577, saving model to /content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_loss_v4.h5\n",
            "26/26 [==============================] - 31s 839ms/step - loss: 0.0774 - accuracy: 0.9747 - val_loss: 0.1658 - val_accuracy: 0.9426 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9809\n",
            "Epoch 2: val_accuracy did not improve from 0.94259\n",
            "\n",
            "Epoch 2: val_loss did not improve from 0.16577\n",
            "26/26 [==============================] - 21s 822ms/step - loss: 0.0803 - accuracy: 0.9809 - val_loss: 0.1900 - val_accuracy: 0.9278 - lr: 9.0484e-04\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9877\n",
            "Epoch 3: val_accuracy improved from 0.94259 to 0.94630, saving model to /content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_acc_v4.h5\n",
            "\n",
            "Epoch 3: val_loss did not improve from 0.16577\n",
            "26/26 [==============================] - 22s 841ms/step - loss: 0.0484 - accuracy: 0.9877 - val_loss: 0.1729 - val_accuracy: 0.9463 - lr: 8.1873e-04\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9963\n",
            "Epoch 4: val_accuracy did not improve from 0.94630\n",
            "\n",
            "Epoch 4: val_loss improved from 0.16577 to 0.16547, saving model to /content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_loss_v4.h5\n",
            "26/26 [==============================] - 22s 845ms/step - loss: 0.0335 - accuracy: 0.9963 - val_loss: 0.1655 - val_accuracy: 0.9463 - lr: 7.4082e-04\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9957\n",
            "Epoch 5: val_accuracy did not improve from 0.94630\n",
            "\n",
            "Epoch 5: val_loss did not improve from 0.16547\n",
            "26/26 [==============================] - 21s 798ms/step - loss: 0.0315 - accuracy: 0.9957 - val_loss: 0.1741 - val_accuracy: 0.9426 - lr: 6.7032e-04\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9988\n",
            "Epoch 6: val_accuracy did not improve from 0.94630\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.16547\n",
            "26/26 [==============================] - 21s 798ms/step - loss: 0.0236 - accuracy: 0.9988 - val_loss: 0.1742 - val_accuracy: 0.9444 - lr: 6.0653e-04\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9988\n",
            "Epoch 7: val_accuracy improved from 0.94630 to 0.95000, saving model to /content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_acc_v4.h5\n",
            "\n",
            "Epoch 7: val_loss improved from 0.16547 to 0.16505, saving model to /content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_loss_v4.h5\n",
            "26/26 [==============================] - 22s 834ms/step - loss: 0.0181 - accuracy: 0.9988 - val_loss: 0.1650 - val_accuracy: 0.9500 - lr: 5.4881e-04\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9988\n",
            "Epoch 8: val_accuracy did not improve from 0.95000\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.16505\n",
            "26/26 [==============================] - 21s 827ms/step - loss: 0.0158 - accuracy: 0.9988 - val_loss: 0.1689 - val_accuracy: 0.9481 - lr: 4.9659e-04\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9988\n",
            "Epoch 9: val_accuracy did not improve from 0.95000\n",
            "\n",
            "Epoch 9: val_loss did not improve from 0.16505\n",
            "26/26 [==============================] - 21s 826ms/step - loss: 0.0147 - accuracy: 0.9988 - val_loss: 0.1755 - val_accuracy: 0.9500 - lr: 4.4933e-04\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9981\n",
            "Epoch 10: val_accuracy did not improve from 0.95000\n",
            "\n",
            "Epoch 10: val_loss did not improve from 0.16505\n",
            "26/26 [==============================] - 21s 798ms/step - loss: 0.0128 - accuracy: 0.9981 - val_loss: 0.1683 - val_accuracy: 0.9500 - lr: 4.0657e-04\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9988\n",
            "Epoch 11: val_accuracy improved from 0.95000 to 0.95185, saving model to /content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_acc_v4.h5\n",
            "\n",
            "Epoch 11: val_loss did not improve from 0.16505\n",
            "26/26 [==============================] - 22s 846ms/step - loss: 0.0114 - accuracy: 0.9988 - val_loss: 0.1678 - val_accuracy: 0.9519 - lr: 3.6788e-04\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9988\n",
            "Epoch 12: val_accuracy improved from 0.95185 to 0.95370, saving model to /content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_acc_v4.h5\n",
            "\n",
            "Epoch 12: val_loss did not improve from 0.16505\n",
            "26/26 [==============================] - 22s 847ms/step - loss: 0.0104 - accuracy: 0.9988 - val_loss: 0.1745 - val_accuracy: 0.9537 - lr: 3.3287e-04\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9988\n",
            "Epoch 13: val_accuracy did not improve from 0.95370\n",
            "\n",
            "Epoch 13: val_loss did not improve from 0.16505\n",
            "26/26 [==============================] - 21s 829ms/step - loss: 0.0086 - accuracy: 0.9988 - val_loss: 0.1707 - val_accuracy: 0.9519 - lr: 3.0119e-04\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9988\n",
            "Epoch 14: val_accuracy did not improve from 0.95370\n",
            "\n",
            "Epoch 14: val_loss did not improve from 0.16505\n",
            "26/26 [==============================] - 21s 828ms/step - loss: 0.0075 - accuracy: 0.9988 - val_loss: 0.1714 - val_accuracy: 0.9519 - lr: 2.7253e-04\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9988\n",
            "Epoch 15: val_accuracy improved from 0.95370 to 0.95556, saving model to /content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_acc_v4.h5\n",
            "\n",
            "Epoch 15: val_loss did not improve from 0.16505\n",
            "26/26 [==============================] - 21s 817ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.1721 - val_accuracy: 0.9556 - lr: 2.4660e-04\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9994\n",
            "Epoch 16: val_accuracy did not improve from 0.95556\n",
            "\n",
            "Epoch 16: val_loss did not improve from 0.16505\n",
            "26/26 [==============================] - 21s 829ms/step - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.1710 - val_accuracy: 0.9519 - lr: 2.2313e-04\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "Epoch 17: val_accuracy improved from 0.95556 to 0.95741, saving model to /content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_acc_v4.h5\n",
            "\n",
            "Epoch 17: val_loss did not improve from 0.16505\n",
            "26/26 [==============================] - 21s 819ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.1724 - val_accuracy: 0.9574 - lr: 2.0190e-04\n",
            "Epoch 17: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_best = load_model('/content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_acc_v4.h5')\n",
        "transformer_accuracy = transformer_best.evaluate(x_data_test, y_data_test, verbose=1)[1]   #테스터 데이터를 사용해 모델 정확도 평가\n",
        "print(\"Test Acc. Transformer: {:.3f}%\".format(transformer_accuracy * 100))\n",
        "\n",
        "transformer_best_loss = load_model('/content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_loss_v4.h5')\n",
        "transformer_loss = transformer_best_loss.evaluate(x_data_test, y_data_test, verbose=1)[0]   #테스터 데이터를 사용해 모델의 손실 평가\n",
        "print(\"Test val_Loss. transformer:{:.3f}\".format(transformer_loss))\n",
        "\n",
        "y_pred = np.array(list(map(lambda x: np.argmax(x), transformer_best.predict(x_data_test))))   #테스트 데이터에 대한 예측 수행 및 confusion matrix 출력하여 성능 평가\n",
        "y_data_test = y_data_test.argmax(axis=1)\n",
        "cm = confusion_matrix(y_data_test, y_pred)\n",
        "print(classification_report(y_data_test, y_pred, digits=4)) #digits=4로 소수점 지정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "23D9PI4CMYal",
        "outputId": "1245860a-1abc-4d34-bb24-9378478f0a7a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1919, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 4) are incompatible\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-001996f80a79>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtransformer_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_acc_v3.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransformer_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m#테스터 데이터를 사용해 모델 정확도 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Acc. Transformer: {:.3f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_accuracy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtransformer_best_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/캡스톤디자인I/Save_Model/best_transformer_loss_v3.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1919, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 4) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zyoC4K52nxhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = trans_h['accuracy']\n",
        "val_acc = trans_h['val_accuracy']\n",
        "loss = trans_h['loss']\n",
        "val_loss = trans_h['val_loss']\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy', color='r')\n",
        "plt.plot(val_acc, label='Validation Accuracy', color='b')\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.legend(loc='lower right', fontsize=13)\n",
        "plt.ylabel('Accuracy', fontsize=16, weight='bold')\n",
        "plt.title('Transformer - Training & Validation Acc.', fontsize=16, weight='bold')\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss', color='r')\n",
        "plt.plot(val_loss, label='Validation Loss', color='b')\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.legend(loc='upper right', fontsize=13)\n",
        "plt.ylabel('Cross Entropy', fontsize=16, weight='bold')\n",
        "plt.title('transformer - Training & Validation Loss', fontsize=15, weight='bold')\n",
        "plt.xlabel('Epoch', fontsize=15, weight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "0IT_9ZXSUyML",
        "outputId": "8a4195cf-ecd9-4833-84d7-7fea1c019ca6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'History' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-056f60d0f33c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'History' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4, restore_best_weights=True)\n",
        "# # filename = os.path.join(model_path, 'checkpoint.h5')\n",
        "# # checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "# model_transformer.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "#               loss=['categorical_crossentropy'], metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "rSdgtJcR0_e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trans_h = model_transformer.fit(x_data, y_data, batch_size=64, epochs=1000, validation_split=0.2, callbacks=[mcp_save_best, mcp_save, csv_logger, Early_stop])"
      ],
      "metadata": {
        "id": "JeYuvikX1Eov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myMdl = tf.keras.models.load_model('/content/drive/MyDrive/캡스톤디자인I/Save_Model/20240409_1modelsave/16-0.98459_best')"
      ],
      "metadata": {
        "id": "_Ue90zK093WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_h = pd.read_csv('/content/drive/MyDrive/캡스톤디자인I/Save_Model/20240409_1modelsave/training.csv')"
      ],
      "metadata": {
        "id": "ljugTQ859-Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss curve\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(trans_h['loss'][:], 'y', label='train loss')\n",
        "plt.plot(trans_h['val_loss'][:], 'r', label='val loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "ILKQcYbU-KOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss curve\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(trans_h['accuracy'][:], 'y', label='train accuracy')\n",
        "plt.plot(trans_h['val_acurracy'][:], 'r', label='val accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "d5kgJk-N-XTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}